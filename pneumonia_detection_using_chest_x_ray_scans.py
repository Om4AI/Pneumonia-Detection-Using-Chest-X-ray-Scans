# -*- coding: utf-8 -*-
"""Pneumonia Detection: Using Chest X-ray scans.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Rn_dJ3SPVUFoTxtCz9RhZmZKwCpzQQZT

# ***Pneumonia Detection: Using Chest X-ray Images***
"""

import os
import tensorflow as tf
from tensorflow import keras
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
from zipfile import ZipFile
import pandas as pd

from keras.preprocessing.image import ImageDataGenerator

# Dataset 

def getkaggledata():
  from google.colab import files
  files.upload()
  !mkdir -p ~/.kaggle
  !cp kaggle.json ~/.kaggle/
  !chmod 600 ~/.kaggle/kaggle.json
  # Kaggle API command for the dataset (Change the API Command here)
  !kaggle datasets download -d paultimothymooney/chest-xray-pneumonia
  # os.rename("face-mask-12k-images-dataset.zip","facemask.zip")

# Get the dataset from Kaggle
getkaggledata()

def extractfromzip(filename):
  with ZipFile(filename,'r') as zip:
    zip.extractall()
    print("Dataset created!")

extractfromzip("chest-xray-pneumonia.zip")

base_dir = "/content/chest_xray/chest_xray"
train_dir = os.path.join(base_dir, "train")
validation_dir = os.path.join(base_dir, "val")
test_dir = os.path.join(base_dir, "test")

"""# ***Data augmentation: Using ImageDataGenerator***"""

train_datagen = ImageDataGenerator(rescale=1./255,
                                   horizontal_flip=True,
                                   fill_mode='nearest',
                                   zoom_range=0.2)

test_datagen = ImageDataGenerator(rescale=1./255)

train_dataset = train_datagen.flow_from_directory(train_dir,
                                                  target_size=(160,160),
                                                  class_mode='categorical',
                                                  batch_size=64)

validation_dataset = test_datagen.flow_from_directory(validation_dir,
                                                  target_size=(160,160),
                                                  class_mode='categorical',
                                                  batch_size=64)

test_dataset = test_datagen.flow_from_directory(test_dir,
                                                  target_size=(160,160),
                                                  class_mode='categorical',
                                                  batch_size=64)

# 0 -- Normal scan  //// 1 -- Pneumoniatic scan
class_names = ['Normal', 'Pneumonia']

# Selects some images from the train_dataset (Iterator object)
images,labels = next(iter(train_dataset))

# Plots the images
plt.figure(figsize=(10,10))
for i in range(9):
    plt.subplot(3,3,i+1)
    plt.imshow(images[i])
    plt.xticks([])
    plt.yticks([])
    if (labels[i]==1.0):plt.xlabel(class_names[1])
    elif (labels[i]==0.0): plt.xlabel(class_names[0])
    

plt.show()



"""# ***Transfer Learning: VGG19***"""

# Load model
base_model = keras.applications.VGG19(include_top=False,
                                      weights='imagenet',
                                      input_shape=(160,160,3))

base_model.trainable= False

base_model.summary()

"""## **Adding a top to the base model**"""

model = keras.Sequential([
          base_model,
          keras.layers.Flatten(),
          keras.layers.Dense(64, activation='relu'),
          keras.layers.Dropout(0.5),
          keras.layers.Dense(2, activation='softmax')
])

model.summary()

model.compile(optimizer=keras.optimizers.Adam(),
              loss=keras.losses.categorical_crossentropy,
              metrics=['accuracy'])

keras.utils.plot_model(model, show_shapes=True)

history = model.fit(train_dataset,
          batch_size=64,
          epochs=5,
          validation_data=validation_dataset)

df = pd.DataFrame(history.history)
df.plot(figsize=(6,6))
plt.legend()



"""## Fine tuning of model"""

len(base_model.layers)

# Unfreeze all the layers
base_model.trainable = True

# Freeze the bottom layers
for layer in base_model.layers[:18]:
  layer.trainable = False

model.summary()



"""## Fine Tuning: Compile & run model again"""

model.compile(optimizer=keras.optimizers.Adam(learning_rate=1e-4),
              loss=keras.losses.categorical_crossentropy,
              metrics=['accuracy'])

total_epochs = 15

es_callback = keras.callbacks.EarlyStopping(patience=3, verbose=1, restore_best_weights=True)

history_fine = model.fit(train_dataset,
                         batch_size=64,
                         epochs=total_epochs,
                         initial_epoch=history.epoch[-1],
                         validation_data=validation_dataset,
                         callbacks=es_callback)

df1 = pd.DataFrame(history_fine.history)
df1.plot(figsize=(6,6))
plt.legend()

import matplotlib.pyplot as plt

def plot_graph(history, word):
  plt.plot(history.history[word])
  plt.plot(history.history['val_'+word])
  plt.xlabel('Epochs')
  plt.ylabel(word)
  plt.legend([word, 'val_'+ word])
  plt.show()

plot_graph(history_fine, 'accuracy')
plot_graph(history_fine, 'loss')



"""## Model Evaluation"""

# Evaluation of model

loss, accuracy = model.evaluate(test_dataset)
print("\n\nLoss: ", loss)
print("Accuracy: {:.2f}".format(accuracy*100),"\n")



"""## Testing the model"""

# Import the required libraries

from google.colab import files
from keras.preprocessing import image

uploaded = files.upload()


for fn in uploaded.keys():

  # print("\n\nThe key is: "+ fn+"\n\n")
  # Predicting images
  # Full path for the image
  path = '/content/' + fn
  # Load the image into the img variable
  img = image.load_img(path, target_size =(160,160))
  x = image.img_to_array(img) # Convert the image into array
  x = np.expand_dims(x, axis=0) # Expand the images as if they were many images

  images = np.vstack([x]) # Important
  classes = model.predict(images, batch_size=10)  # Gives the predictions for each classes like: eg:  [classes[0]: ([ 3.729408   -0.32879975  0.60124546 -3.0891216   0.56141305])]
  score = tf.nn.softmax(classes[0]) #Put the predictions into a softmax layer to get a [score: (tf.tensor)]


  # np.argmax(): Indicies of the max of the predictions
  # Finally to get the name of the predicted class we write the next line i.e the class_names[index(found above)]
  name = class_names[np.argmax(score)]
  accuracy = 100 * np.max(score) # Accuracy percentage

  plt.imshow(img)
  plt.axis("off")
  print("\n\nFinal prediction: ")
  print("The image is most likely "+ name +" with the accuracy of " + str(accuracy) )

"""## Saving the model"""

# Saving model

model.save("Pneumonia_detection.h5")



"""## Testing using Saved Model"""

# Import the required libraries

from google.colab import files
from keras.preprocessing import image

uploaded = files.upload()


for fn in uploaded.keys():

  # print("\n\nThe key is: "+ fn+"\n\n")
  # Predicting images
  # Full path for the image
  path = '/content/' + fn
  # Load the image into the img variable
  img = image.load_img(path, target_size =(160,160))
  x = image.img_to_array(img) # Convert the image into array
  x = np.expand_dims(x, axis=0) # Expand the images as if they were many images

  # Importing model
  model1 = keras.models.load_model("Pneumonia_detection.h5")

  images = np.vstack([x]) # Important
  classes = model1.predict(images, batch_size=10)  # Gives the predictions for each classes like: eg:  [classes[0]: ([ 3.729408   -0.32879975  0.60124546 -3.0891216   0.56141305])]
  score = tf.nn.softmax(classes[0]) #Put the predictions into a softmax layer to get a [score: (tf.tensor)]


  # np.argmax(): Indicies of the max of the predictions
  # Finally to get the name of the predicted class we write the next line i.e the class_names[index(found above)]
  name = class_names[np.argmax(score)]
  accuracy = 100 * np.max(score) # Accuracy percentage

  plt.imshow(img)
  plt.axis("off")
  print("\n\nFinal prediction: ")
  print("The image is most likely "+ name +" with the accuracy of " + str(accuracy) )

